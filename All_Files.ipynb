{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb74a62f-a4a9-4e61-9415-58c8a06f5f75",
   "metadata": {},
   "source": [
    "# Hit classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61cb5d-7c43-4a94-9e3c-eb2889e69b88",
   "metadata": {},
   "source": [
    "In this notebook we will try to use Keras and XGBoost in order to distinguish between _true_ conversion electron hits and _fake_ conversion electron hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c327bc6-5090-46b7-83a0-3b931273f9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ce5426-86e5-4f6e-a7d5-a847d6e2ce97",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'uproot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muproot\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mawkward\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mak\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'uproot'"
     ]
    }
   ],
   "source": [
    "import uproot \n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc82b708-2543-44d2-9b5d-ca958c5b65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b18db5-7d46-40ed-9a1b-528d48256097",
   "metadata": {},
   "source": [
    "First of all we use [uproot](https://uproot.readthedocs.io/en/latest/), a library to reading ROOT files in pure Python and NumPy, to open the `trkana` tree in the `TAKK` folder of the `KKSM01.root` file. We only need certain branches, so we apply a filter to read only `de`, `detsh`, `detshmc`, and `demc`. We then apply a mask to our tree to select only good fits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afa96c-4517-4bca-95c5-53ef01a36dc8",
   "metadata": {},
   "source": [
    "Then, we select the first 10000 events in the tree and concatenate the lists of the hit variables in a single, large array. These 5 arrays are then stacked in a single bi-dimensional array with `np.vstack`, which will be our input dataset used for the training of the machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83ff4566-18f2-419a-af8b-679ffe222187",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m input_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mempty\n\u001b[1;32m      3\u001b[0m temp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty\n\u001b[1;32m      4\u001b[0m mcrel \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "input_dataset = np.empty\n",
    "temp = np.empty\n",
    "mcrel = []\n",
    "n_events = 100000\n",
    "\n",
    "for filename in os.listdir(\"../TrkAna/39501509\"): # CHANGE TO GLOBAL/ENVIRONMENT VARIABLE!\n",
    "    with uproot.open(\"../TrkAna/39501509/\" + filename) as file:\n",
    "        trkana = file[\"TAKK\"][\"trkana\"].arrays(filter_name=\"/de|detsh|detshmc|demc/i\")\n",
    "        trkana = trkana[(trkana['de.goodfit']==1)&(trkana['de.status']>0)&(trkana['demc.proc']==167)]\n",
    "        \n",
    "    print(\"Processing: \" + filename)    \n",
    "    udt = np.add(ak.concatenate(trkana['detsh.udt'][:n_events]).to_numpy(), ak.concatenate(trkana['detsh.udt'][:n_events]).to_numpy())  \n",
    "    udoca = ak.concatenate(trkana['detsh.udoca'][:n_events]).to_numpy()\n",
    "    tottdrift = ak.concatenate(trkana['detsh.tottdrift'][:n_events]).to_numpy()\n",
    "    rdrift = ak.concatenate(trkana['detsh.rdrift'][:n_events]).to_numpy()\n",
    "    edep = ak.concatenate(trkana['detsh.edep'][:n_events]).to_numpy()\n",
    "    udocavar = ak.concatenate(trkana['detsh.udocavar'][:n_events]).to_numpy()\n",
    "    wdist = ak.concatenate(trkana['detsh.wdist'][:n_events]).to_numpy()\n",
    "    uupos = ak.concatenate(trkana['detsh.uupos'][:n_events]).to_numpy()\n",
    "    rho = np.square(ak.concatenate(trkana['detsh.poca.fCoordinates.fX'][:n_events]).to_numpy())\n",
    "    rho = np.add(rho,np.square(ak.concatenate(trkana['detsh.poca.fCoordinates.fY'][:n_events]).to_numpy()))\n",
    "    rho = np.sqrt(rho)\n",
    "    \n",
    "    temp = np.vstack((udt,udoca,tottdrift,rdrift,edep, udocavar, wdist, uupos, rho)).T\n",
    "    if input_dataset is np.empty:\n",
    "        input_dataset = temp\n",
    "    else:\n",
    "        input_dataset = np.concatenate((input_dataset, temp))\n",
    "        \n",
    "    for i, this_dt in enumerate(trkana['detsh.udt'][:n_events]):\n",
    "        mcrel.extend(trkana['detshmc.rel._rel'][i][:len(this_dt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8089c603-34e9-45d2-9a70-d6f8f42c4e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142593741"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dataset.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167fc47-dd84-4a31-98d0-a0270f04bfd5",
   "metadata": {},
   "source": [
    "Here we then assign a label to each hit as _signal_ or _background_, depending on the Monte Carlo truth information. Since the dimension of  `detshmc.rel._rel` is not guaranteed to be the same as the dimension of `detsh` we need to loop over all the entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e546d312-4d8b-4757-9b0e-7293d9eae321",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcrel = np.array(mcrel)\n",
    "true_ce = mcrel==0\n",
    "    \n",
    "signal = true_ce\n",
    "bkg = mcrel ==-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2e8445-92ff-416f-a9a8-2d071b1f0bb9",
   "metadata": {},
   "source": [
    "In our problem we have a different number of signal and background entries in our input dataset. There are several techniques avaialable for _unbalanced_ datasets. Here we are using the most naive one, which is just using $\\min(N_{sig}, N_{bkg})$ events. Then, we divide our input into the _training_, _validation_, and _test_ datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed7b23e-d3ab-4645-a905-c3d4090eb7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = min(len(input_dataset[signal]), len(input_dataset[bkg]))\n",
    "signal_dataset = input_dataset[signal][:min_len]\n",
    "bkg_dataset = input_dataset[bkg][:min_len]\n",
    "\n",
    "balanced_input = np.concatenate((signal_dataset, bkg_dataset))\n",
    "y_balanced_input = np.concatenate((np.ones(signal_dataset.shape[0]), np.zeros(bkg_dataset.shape[0])))\n",
    "\n",
    "n_variables = balanced_input.shape[1]\n",
    "\n",
    "x_ce_train, x_ce_test, y_ce_train, y_ce_test = train_test_split(balanced_input, y_balanced_input, test_size=0.5, random_state=42)\n",
    "x_ce_test, x_ce_valid, y_ce_test, y_ce_valid = train_test_split(x_ce_test, y_ce_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414715a1-8fe6-47d2-b856-76e7f09a30c5",
   "metadata": {},
   "source": [
    "## Create and train a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3968ba-51a2-45c8-9882-c989a3f7d691",
   "metadata": {},
   "source": [
    "Here we create a _multi-layer perceptron_ (MLP) model which consists of 3 fully-connected (or _dense_) layers, each one followed by a _dropout_ layer, which helps to avoid overfitting. The model is trained using the [Adam](https://arxiv.org/abs/1412.6980) optimizer and trained for 50 epochs or until the validation loss doesn't improve for 5 epochs (`early_stop`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be3a800-39dd-41ff-bd78-c5156fd4919c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50606/3427856685.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"logs/fit/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d-%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtensorboard_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistogram_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mearly_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tensorflow' is not defined"
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate=1e-3)\n",
    "\n",
    "model_ce = Sequential()\n",
    "model_ce.add(Dense(n_variables+1, activation='relu', input_shape=(n_variables,)))\n",
    "# model_ce.add(Dropout(0.2))\n",
    "model_ce.add(Dense(n_variables+1, activation='relu'))\n",
    "# model_ce.add(Dropout(0.2))\n",
    "model_ce.add(Dense(n_variables+1, activation='relu'))\n",
    "# model_ce.add(Dropout(0.2))\n",
    "model_ce.add(Dense(1, activation='sigmoid'))\n",
    "model_ce.compile(loss='binary_crossentropy',\n",
    "                 metrics='accuracy',\n",
    "                 optimizer=opt)\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tensorflow.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# history_ce = model_ce.fit(x_ce_train, y_ce_train,\n",
    "#                           epochs=50,\n",
    "#                           verbose=1,\n",
    "#                           validation_data=(x_ce_valid, y_ce_valid),\n",
    "#                           callbacks=[early_stop])\n",
    "\n",
    "history_ce = model_ce.fit(x_ce_train, y_ce_train,\n",
    "                          epochs=50,\n",
    "                          verbose=1,\n",
    "                          validation_data=(x_ce_valid, y_ce_valid),\n",
    "                          callbacks=[tensorboard_callback])\n",
    "\n",
    "model_ce.save(\"KKtrainModel.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_ce.history['val_loss'],label=\"val loss\")\n",
    "plt.plot(history_ce.history['loss'],label=\"loss\")\n",
    "plt.legend()\n",
    "\n",
    "%load_ext tensorboard\n",
    "#rm -rf ./logs/\n",
    "%tensorboard --logdir logs/fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78fb06e-22e5-4ecb-b2c5-48b591268733",
   "metadata": {},
   "source": [
    "## Create and train a Boosted Decision Tree\n",
    "Here, instead of using a MLP, we use a [_Gradient Boosted Decision Tree_](https://xgboost.readthedocs.io/en/stable/) (BDT) to distinguish between signal (true CE hits) and background (fake CE hits). We use the defualt hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e5894-b4e3-405c-a99b-c76b2f12403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgboost = XGBClassifier()\n",
    "model_xgboost.fit(x_ce_train, y_ce_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa6212-21f7-46c8-b7af-dab07ced4269",
   "metadata": {},
   "source": [
    "Here we can finally apply our two models (the MLP and the BDT) to our test datasets and create the corresponding ROC curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbef70-e875-40dd-8a8f-5a66fc32fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_ce = model_ce.predict(x_ce_test).ravel()\n",
    "fpr_ce, tpr_ce, th_ce = roc_curve(y_ce_test,  prediction_ce)\n",
    "auc_ce = roc_auc_score(y_ce_test, prediction_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6a838-1a4c-41e3-a2bb-46a270ad6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_xgboost = model_xgboost.predict_proba(x_ce_test)[:,1]\n",
    "fpr_xgboost, tpr_xgboost, th_xgboost = roc_curve(y_ce_test,  prediction_xgboost)\n",
    "auc_xgboost = roc_auc_score(y_ce_test, prediction_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f392a2-223f-4a47-81de-45dc56b8e42d",
   "metadata": {},
   "source": [
    "The plot of the ROC curves clearly shows that the BDT outperforms the MLP. In principle, however, it should be possible to improve the MLP performances by optimizing the hyperparameters (learning rate, hidden layers, activation functions, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9b80f-d130-4ac0-bc19-ea03de7032f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(tpr_ce,1-fpr_ce,label=f'MLP AUC: {auc_ce:.2f}')\n",
    "ax.plot(tpr_xgboost,1-fpr_xgboost,label=f'BDT AUC: {auc_xgboost:.2f}')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel(\"Signal efficiency\")\n",
    "ax.set_ylabel(\"Background rejection\")\n",
    "ax.set_xlim(0,1.05)\n",
    "ax.set_ylim(0,1.05)\n",
    "fig.savefig(\"truece.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
